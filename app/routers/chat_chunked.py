"""
Router chat MCP avec chunking intelligent int√©gr√©
Permet des analyses SEO compl√®tes malgr√© les limites de tokens OpenAI
"""
import json
import asyncio
from datetime import datetime
from typing import List, Dict, Any, Optional
from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel, Field
from openai import AsyncOpenAI

from ..config import Config
from ..dependencies import get_haloscan_client, HaloscanClient
from ..logging_config import get_logger
from ..mcp_tools.registry import global_mcp_registry
from ..services.chunking_service import chunking_service, ChunkProcessor
from .chat import ChatMessage, ChatRequest, ChatResponse, get_haloscan_tools, execute_haloscan_tool

logger = get_logger("chat_chunked")
router = APIRouter(tags=["chat-chunked"])

# Configuration OpenAI optimis√©e pour le maximum d'informations
openai_client = AsyncOpenAI(api_key=Config.OPENAI_API_KEY)

def detect_complex_analysis(message: str) -> bool:
    """D√©tecte si la requ√™te n√©cessite une analyse complexe (chunking)"""
    complex_keywords = [
        "analyse compl√®te", "analyse d√©taill√©e", "tous les concurrents", 
        "analyse exhaustive", "√©tude approfondie", "rapport complet",
        "tous les mots-cl√©s", "analyse concurrentielle", "audit complet"
    ]
    return any(keyword in message.lower() for keyword in complex_keywords)

async def perform_chunked_analysis(
    request: ChatRequest, 
    client: HaloscanClient
) -> ChatResponse:
    """Effectue une analyse chunked compl√®te - VERSION SIMPLIFI√âE FONCTIONNELLE"""
    
    # 1. Extraire le domaine
    domain = extract_domain_from_query(request.message)
    if not domain:
        raise HTTPException(400, "Impossible d'extraire le domaine de la requ√™te")
    
    logger.info(f"[{datetime.now().strftime('%H:%M:%S')}] CHUNKING: Analyse complexe d√©tect√©e pour {domain}")
    
    try:
        # 2. Collecter toutes les donn√©es n√©cessaires
        logger.info(f"[{datetime.now().strftime('%H:%M:%S')}] CHUNKING: Collecte des donn√©es...")
        
        # Analyse du domaine principal
        domain_analysis = await execute_haloscan_tool("analyze_domain", {"domain": domain}, client)
        
        # Analyse des concurrents
        competitors_analysis = await execute_haloscan_tool("find_domain_competitors", {"domain": domain}, client)
        
        # Mots-cl√©s par positions (focus sur page 2 comme demand√©) - LIMITE AUGMENT√âE
        keywords_page2 = await execute_haloscan_tool("search_keywords_by_position", {
            "domain": domain, 
            "position_min": 11, 
            "position_max": 20,
            "limit": 200  # Augmenter la limite pour plus de mots-cl√©s
        }, client)
        
        # Mots-cl√©s page 1 pour comparaison - LIMITE AUGMENT√âE
        keywords_page1 = await execute_haloscan_tool("search_keywords_by_position", {
            "domain": domain, 
            "position_min": 1, 
            "position_max": 10,
            "limit": 200  # Augmenter la limite pour plus de mots-cl√©s
        }, client)
        
        # R√©cup√©rer aussi les positions 21-50 pour une analyse encore plus compl√®te
        keywords_page3_plus = await execute_haloscan_tool("search_keywords_by_position", {
            "domain": domain, 
            "position_min": 21, 
            "position_max": 50,
            "limit": 200  # Positions plus lointaines mais importantes
        }, client)
        
        logger.info(f"[{datetime.now().strftime('%H:%M:%S')}] CHUNKING: Donn√©es collect√©es, g√©n√©ration de la synth√®se...")
        
        # 3. G√©n√©rer la synth√®se finale avec OpenAI optimis√©
        synthesis_prompt = f"""ANALYSE SEO COMPL√àTE - SYNTH√àSE FINALE

üéØ DOMAINE ANALYS√â: {domain}
üìä DONN√âES COLLECT√âES:
- Analyse du domaine principal
- {len(competitors_analysis.get('competitors', []))} concurrents identifi√©s
- Mots-cl√©s positions 1-10 (page 1)
- Mots-cl√©s positions 11-20 (page 2) - FOCUS PRINCIPAL
- Mots-cl√©s positions 21-50 (pages 3+) - ANALYSE COMPL√àTE

üéØ MISSION: Cr√©er une √©tude de mots-cl√©s compl√®te au format demand√©:
keyword | volume | difficulty | competition | cpc | trend | thematic_cluster | sous_thematique | intention_type

‚ö†Ô∏è INSTRUCTION CRITIQUE: AFFICHE TOUS LES MOTS-CL√âS TROUV√âS (PAS SEULEMENT LES 20 PREMIERS)
Tu dois inclure TOUS les mots-cl√©s des 3 datasets dans ton tableau final. Ne limite pas √† 20 lignes !

üìã DONN√âES BRUTES:

DOMAINE PRINCIPAL:
{json.dumps(domain_analysis, indent=2)}

CONCURRENTS:
{json.dumps(competitors_analysis, indent=2)}

MOTS-CL√âS PAGE 2 (11-20) - PRIORIT√â:
{json.dumps(keywords_page2, indent=2)}

MOTS-CL√âS PAGE 1 (1-10) - R√âF√âRENCE:
{json.dumps(keywords_page1, indent=2)}

MOTS-CL√âS PAGES 3+ (21-50) - ANALYSE COMPL√àTE:
{json.dumps(keywords_page3_plus, indent=2)}

üéØ INSTRUCTIONS CRITIQUES:
1. FOCUS sur les mots-cl√©s o√π {domain} se positionne APR√àS la page 1 (positions 11-20)
2. Analyser les concurrents qui se positionnent mieux sur ces mots-cl√©s
3. Structurer OBLIGATOIREMENT au format: keyword | volume | difficulty | competition | cpc | trend | thematic_cluster | sous_thematique | intention_type
4. Fournir des recommandations actionables
5. √ätre exhaustif et d√©taill√©

‚ö†Ô∏è FORMAT DE R√âPONSE OBLIGATOIRE:
- PAS DE BLABLA, PAS D'INTRODUCTION, PAS DE CONCLUSION
- UNIQUEMENT LE TABLEAU FORMAT√â EN MARKDOWN
- √âCONOMISER LES TOKENS POUR MAXIMISER LES DONN√âES
- COMMENCER DIRECTEMENT PAR L'EN-T√äTE DU TABLEAU

G√âN√àRE UNIQUEMENT LE TABLEAU:"""
        
        # Appel OpenAI optimis√© pour maximum d'informations et TOUS les mots-cl√©s
        response = await openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Tu es un expert SEO qui g√©n√®re des analyses compl√®tes et d√©taill√©es. Tu dois TOUJOURS respecter le format demand√©, √™tre exhaustif, et AFFICHER TOUS LES MOTS-CL√âS TROUV√âS sans limitation. Ne limite jamais le nombre de lignes dans le tableau final."},
                {"role": "user", "content": synthesis_prompt}
            ],
            max_tokens=8000,  # AUGMENT√â pour permettre plus de mots-cl√©s
            temperature=0.1,  # Pr√©cision maximale pour les donn√©es
            top_p=0.85  # Focus sur les r√©ponses les plus probables
        )
        
        final_analysis = response.choices[0].message.content
        
        logger.info(f"[{datetime.now().strftime('%H:%M:%S')}] CHUNKING: Analyse termin√©e avec succ√®s")
        
        # 4. Construire la r√©ponse finale
        return ChatResponse(
            response=final_analysis,
            tools_used=[
                {"tool": "analyze_domain", "success": True},
                {"tool": "find_domain_competitors", "success": True, "chunks_processed": len(competitors_analysis.get('competitors', []))},
                {"tool": "search_keywords_by_position", "success": True, "chunks_processed": 3, "details": "Positions 1-10, 11-20, 21-50"},
                {"tool": "chunked_synthesis", "success": True, "chunks_processed": 1, "max_tokens": 8000}
            ],
            conversation_history=request.conversation_history + [
                ChatMessage(role="user", content=request.message),
                ChatMessage(role="assistant", content=final_analysis)
            ]
        )
        
    except Exception as e:
        logger.error(f"[{datetime.now().strftime('%H:%M:%S')}] CHUNKING: Erreur - {str(e)}")
        
        # Fallback sur le chat normal en cas d'erreur
        from .chat import chat_with_mcp
        return await chat_with_mcp(request, client)

async def get_comprehensive_competitor_data(domain: str, client: HaloscanClient) -> List[Dict]:
    """R√©cup√®re toutes les donn√©es concurrents disponibles"""
    try:
        # Utiliser l'outil find_domain_competitors
        competitors_result = await execute_haloscan_tool("find_domain_competitors", {"domain": domain}, client)
        
        if competitors_result.get("success"):
            competitors = competitors_result.get("data", {}).get("competitors", [])
            logger.info(f"[{datetime.now().strftime('%H:%M:%S')}] DATA: {len(competitors)} concurrents trouv√©s pour {domain}")
            return competitors[:15]  # Limiter √† 15 concurrents max
        
        return []
    except Exception as e:
        logger.error(f"Erreur r√©cup√©ration concurrents: {e}")
        return []

async def get_comprehensive_keyword_data(domain: str, client: HaloscanClient) -> List[Dict]:
    """R√©cup√®re toutes les donn√©es mots-cl√©s disponibles"""
    try:
        # Utiliser l'outil search_keywords_by_position pour diff√©rentes plages
        all_keywords = []
        
        # Positions 1-10 (page 1)
        result_p1 = await execute_haloscan_tool("search_keywords_by_position", {
            "domain": domain, "position_min": 1, "position_max": 10
        }, client)
        
        # Positions 11-20 (page 2)
        result_p2 = await execute_haloscan_tool("search_keywords_by_position", {
            "domain": domain, "position_min": 11, "position_max": 20
        }, client)
        
        # Positions 21-50 (pages 3-5)
        result_p3 = await execute_haloscan_tool("search_keywords_by_position", {
            "domain": domain, "position_min": 21, "position_max": 50
        }, client)
        
        # Combiner tous les r√©sultats
        for result in [result_p1, result_p2, result_p3]:
            if result.get("success"):
                keywords = result.get("data", {}).get("keywords", [])
                all_keywords.extend(keywords)
        
        logger.info(f"[{datetime.now().strftime('%H:%M:%S')}] DATA: {len(all_keywords)} mots-cl√©s trouv√©s pour {domain}")
        return all_keywords[:200]  # Limiter √† 200 mots-cl√©s max
        
    except Exception as e:
        logger.error(f"Erreur r√©cup√©ration mots-cl√©s: {e}")
        return []

def create_competitor_chunks(competitors: List[Dict], chunk_size: int = 3) -> List[List[Dict]]:
    """Cr√©e des chunks intelligents de concurrents"""
    if not competitors:
        return []
    
    chunks = []
    for i in range(0, len(competitors), chunk_size):
        chunk = competitors[i:i + chunk_size]
        chunks.append(chunk)
    
    return chunks

def create_keyword_chunks(keywords: List[Dict], chunk_size: int = 30) -> List[List[Dict]]:
    """Cr√©e des chunks intelligents de mots-cl√©s"""
    if not keywords:
        return []
    
    # Trier par volume/importance
    sorted_keywords = sorted(keywords, key=lambda k: k.get('volume', 0), reverse=True)
    
    chunks = []
    for i in range(0, len(sorted_keywords), chunk_size):
        chunk = sorted_keywords[i:i + chunk_size]
        chunks.append(chunk)
    
    return chunks

async def process_competitor_chunk(
    competitors: List[Dict], 
    domain: str, 
    chunk_num: int, 
    total_chunks: int
) -> Dict[str, Any]:
    """Traite un chunk de concurrents avec OpenAI optimis√©"""
    
    competitor_names = [comp.get("domain", "N/A") for comp in competitors]
    
    prompt = f"""ANALYSE SEO CONCURRENTIELLE - √âTAPE {chunk_num}/{total_chunks}

üéØ DOMAINE ANALYS√â: {domain}
üè¢ CONCURRENTS: {', '.join(competitor_names)}

üìä DONN√âES D√âTAILL√âES:
{json.dumps(competitors, indent=2, ensure_ascii=False)}

üéØ MISSION CRITIQUE: Analyse exhaustive de ces concurrents

ANALYSE REQUISE (SOIS TR√àS D√âTAILL√â):

1. üîç MOTS-CL√âS UNIQUES PAR CONCURRENT:
   - Liste pr√©cise des mots-cl√©s que chaque concurrent cible
   - Volumes de recherche et difficult√©s
   - Mots-cl√©s que {domain} ne cible PAS

2. üìà STRAT√âGIES SEO IDENTIFI√âES:
   - Approches de contenu sp√©cifiques
   - Structures de site observ√©es
   - Tactiques de positionnement

3. üéØ OPPORTUNIT√âS CONCR√àTES:
   - Gaps exploitables imm√©diatement
   - Mots-cl√©s √† faible concurrence
   - Niches non exploit√©es

4. ‚ö° INSIGHTS ACTIONNABLES:
   - Actions prioritaires pour {domain}
   - Recommandations sp√©cifiques
   - M√©triques √† surveiller

IMPORTANT: 
- Fournis un maximum de d√©tails et de donn√©es concr√®tes
- Chaque insight doit √™tre actionnable
- Cette analyse sera combin√©e avec d'autres chunks
- Termine par 5 insights cl√©s pour la synth√®se finale"""

    try:
        response = await openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,  # Plus bas pour plus de pr√©cision
            max_tokens=3000,  # Maximum pour plus d'infos
            top_p=0.9,
            frequency_penalty=0.1,
            presence_penalty=0.1
        )
        
        content = response.choices[0].message.content
        
        # Extraire les insights cl√©s (derni√®res lignes)
        insights = extract_key_insights(content)
        
        return {
            "type": "competitor_analysis",
            "chunk_num": chunk_num,
            "content": content,
            "summary": f"Analyse de {len(competitors)} concurrents: {', '.join(competitor_names)}",
            "insights": insights
        }
        
    except Exception as e:
        logger.error(f"Erreur traitement chunk concurrent: {e}")
        return {
            "type": "competitor_analysis",
            "chunk_num": chunk_num,
            "content": f"Erreur lors de l'analyse des concurrents: {str(e)}",
            "summary": f"Erreur chunk {chunk_num}",
            "insights": []
        }

async def process_keyword_chunk(
    keywords: List[Dict], 
    domain: str, 
    chunk_num: int, 
    total_chunks: int
) -> Dict[str, Any]:
    """Traite un chunk de mots-cl√©s avec OpenAI optimis√©"""
    
    prompt = f"""ANALYSE SEO MOTS-CL√âS - √âTAPE {chunk_num}/{total_chunks}

üéØ DOMAINE ANALYS√â: {domain}
üìä MOTS-CL√âS: {len(keywords)} mots-cl√©s √† analyser

üìà DONN√âES COMPL√àTES:
{json.dumps(keywords[:20], indent=2, ensure_ascii=False)}
{"... et " + str(len(keywords)-20) + " autres mots-cl√©s" if len(keywords) > 20 else ""}

üéØ MISSION CRITIQUE: Analyse exhaustive de ces mots-cl√©s

ANALYSE REQUISE (SOIS TR√àS D√âTAILL√â):

1. üèÜ OPPORTUNIT√âS PRIORITAIRES:
   - Top 10 des mots-cl√©s √† fort potentiel
   - Ratio volume/difficult√© optimal
   - Estimations de trafic potentiel

2. üìù GAPS DE CONTENU:
   - Sujets non couverts par {domain}
   - Types de contenu manquants
   - Intentions de recherche non adress√©es

3. üìÖ ANALYSE TEMPORELLE:
   - Tendances saisonni√®res d√©tect√©es
   - √âvolutions de volume observ√©es
   - Moments optimaux pour cibler

4. üéØ CLUSTERING TH√âMATIQUE:
   - Groupes de mots-cl√©s coh√©rents
   - Piliers de contenu sugg√©r√©s
   - Architecture SEO recommand√©e

5. ‚ö° PLAN D'ACTION:
   - Priorisation par impact/effort
   - Recommandations de contenu
   - M√©triques de suivi

IMPORTANT:
- Fournis des donn√©es chiffr√©es pr√©cises
- Chaque recommandation doit √™tre concr√®te
- Cette analyse sera combin√©e avec d'autres chunks
- Termine par 5 actions prioritaires pour la synth√®se finale"""

    try:
        response = await openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,  # Plus bas pour plus de pr√©cision
            max_tokens=3000,  # Maximum pour plus d'infos
            top_p=0.9,
            frequency_penalty=0.1,
            presence_penalty=0.1
        )
        
        content = response.choices[0].message.content
        
        # Extraire les insights cl√©s
        insights = extract_key_insights(content)
        
        return {
            "type": "keyword_analysis",
            "chunk_num": chunk_num,
            "content": content,
            "summary": f"Analyse de {len(keywords)} mots-cl√©s (positions vari√©es)",
            "insights": insights
        }
        
    except Exception as e:
        logger.error(f"Erreur traitement chunk mots-cl√©s: {e}")
        return {
            "type": "keyword_analysis",
            "chunk_num": chunk_num,
            "content": f"Erreur lors de l'analyse des mots-cl√©s: {str(e)}",
            "summary": f"Erreur chunk {chunk_num}",
            "insights": []
        }

async def perform_final_synthesis(analysis_id: str) -> Dict[str, Any]:
    """Effectue la synth√®se finale de tous les chunks"""
    
    synthesis_prompt = chunking_service.get_synthesis_prompt(analysis_id)
    
    # Prompt optimis√© pour la synth√®se finale
    final_prompt = f"""{synthesis_prompt}

üéØ SYNTH√àSE FINALE EXHAUSTIVE REQUISE:

Cr√©e maintenant un rapport SEO complet et actionnable qui inclut:

1. üìä R√âSUM√â EX√âCUTIF:
   - Situation actuelle du domaine
   - Principales d√©couvertes
   - Potentiel d'am√©lioration chiffr√©

2. üèÜ OPPORTUNIT√âS PRIORITAIRES (TOP 10):
   - Mots-cl√©s √† cibler en priorit√©
   - Estimations de trafic potentiel
   - Niveau de difficult√© et ressources n√©cessaires

3. üéØ PLAN D'ACTION D√âTAILL√â:
   - Actions imm√©diates (0-30 jours)
   - Actions moyen terme (1-3 mois)
   - Actions long terme (3-12 mois)

4. üìà M√âTRIQUES DE SUIVI:
   - KPIs √† surveiller
   - Outils de mesure recommand√©s
   - Fr√©quence de reporting

5. üí° RECOMMANDATIONS STRAT√âGIQUES:
   - Axes de d√©veloppement prioritaires
   - Investissements conseill√©s
   - Risques √† √©viter

IMPORTANT: Sois exhaustif, pr√©cis et actionnable. Ce rapport doit permettre une mise en ≈ìuvre imm√©diate."""

    try:
        response = await openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": final_prompt}],
            temperature=0.2,  # Tr√®s bas pour coh√©rence maximale
            max_tokens=4000,  # Maximum absolu pour synth√®se compl√®te
            top_p=0.8,
            frequency_penalty=0.0,
            presence_penalty=0.0
        )
        
        content = response.choices[0].message.content
        
        return {
            "type": "final_synthesis",
            "content": content
        }
        
    except Exception as e:
        logger.error(f"Erreur synth√®se finale: {e}")
        return {
            "type": "final_synthesis", 
            "content": f"Erreur lors de la synth√®se finale: {str(e)}"
        }

def extract_domain_from_query(query: str) -> Optional[str]:
    """Extrait le domaine de la requ√™te utilisateur"""
    import re
    
    # Rechercher des patterns de domaine
    domain_patterns = [
        r'https?://(?:www\.)?([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
        r'(?:www\.)?([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
        r'([a-zA-Z0-9.-]+\.(?:com|fr|org|net|info|biz))'
    ]
    
    for pattern in domain_patterns:
        matches = re.findall(pattern, query.lower())
        if matches:
            return matches[0].replace('www.', '')
    
    return None

def extract_key_insights(content: str) -> List[str]:
    """Extrait les insights cl√©s du contenu g√©n√©r√©"""
    lines = content.split('\n')
    insights = []
    
    # Chercher les lignes qui ressemblent √† des insights
    for line in lines:
        line = line.strip()
        if any(marker in line.lower() for marker in ['insight', 'recommandation', 'action', 'priorit√©', '‚Ä¢', '-']):
            if len(line) > 20:  # √âviter les lignes trop courtes
                insights.append(line)
    
    return insights[:10]  # Limiter √† 10 insights max

# Endpoint principal avec chunking intelligent
@router.post("/chat-chunked", response_model=ChatResponse)
async def chat_with_chunking(
    request: ChatRequest,
    client: HaloscanClient = Depends(get_haloscan_client)
) -> ChatResponse:
    """
    üß† Interface chat MCP avec chunking intelligent
    
    TOUJOURS utilise le chunking intelligent - pas de d√©tection n√©cessaire
    """
    
    # TOUJOURS utiliser le chunking intelligent sur cet endpoint
    logger.info(f"[{datetime.now().strftime('%H:%M:%S')}] CHUNKING: Mode chunking intelligent activ√©")
    return await perform_chunked_analysis(request, client)
